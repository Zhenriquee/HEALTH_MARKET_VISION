{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhenriquee/ANALISE_OPERADORAS/blob/main/Extra%C3%A7%C3%A3o_e_Tratamento_dos_Dados_ANS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cxjhRlYP6JT"
      },
      "source": [
        "# Coleta e Tratamento de Dados da ANS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCC815l2RBWB"
      },
      "source": [
        "## Objetivo:\n",
        "Realizar a extração dos dados da ANS com a finalidade de analisar o posicionamento da Unimed Caruaru com Relação as outras Operadoras, no final iremos armazenar esses dados em um arquivo .db e Utilizar o streamlit para projeção desses dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvLj1QOxP6Gq"
      },
      "source": [
        "## Links Utilizados para Extração\n",
        "\n",
        "\n",
        "*   [Qtd. Beneficiarios por Trimestre](https://dadosabertos.ans.gov.br/FTP/Base_de_dados/Microdados/dados_dbc/beneficiarios/operadoras/)\n",
        "*   [Demonstração Contabeis](https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83gT3xLJYs1t"
      },
      "source": [
        "### Bibliotecas Utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install datasus_dbc dbfread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qo5p9oOYwF2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import uuid\n",
        "import re\n",
        "import io\n",
        "import zipfile\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from datasus_dbc import decompress\n",
        "from dbfread import DBF\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD1UCLoJP6EF"
      },
      "source": [
        "## Inicio Extração e Tratamento Qtd. Beneficiarios por Trimestre\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-hknZKxLRcmx"
      },
      "outputs": [],
      "source": [
        "# --- FUNÇÕES AUXILIARES (Worker) ---\n",
        "# Precisam estar fora da classe para o multiprocessing funcionar bem no Windows\n",
        "\n",
        "def _gerar_chave_trimestre(id_cmpt):\n",
        "    try:\n",
        "        s_cmpt = str(id_cmpt).strip()\n",
        "        if len(s_cmpt) < 6: return None\n",
        "        ano = s_cmpt[:4]\n",
        "        mes = int(s_cmpt[4:6])\n",
        "        trimestre = (mes - 1) // 3 + 1\n",
        "        return f\"{ano}-T{trimestre}\"\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def processar_arquivo_worker(link):\n",
        "    \"\"\"\n",
        "    Função isolada que roda em um núcleo separado da CPU.\n",
        "    Baixa, Converte, Filtra e Agrupa. Retorna um DataFrame pronto (ou None).\n",
        "    \"\"\"\n",
        "    nome_arquivo = link.split('/')[-1]\n",
        "\n",
        "    # Gera nomes únicos para evitar colisão entre processos\n",
        "    id_unico = str(uuid.uuid4())\n",
        "    temp_dbc = f\"temp_{id_unico}.dbc\"\n",
        "    temp_dbf = f\"temp_{id_unico}.dbf\"\n",
        "\n",
        "    colunas_desejadas = ['ID_CMPT', 'CD_OPERADO', 'NR_BENEF_T']\n",
        "    resultado_df = None\n",
        "\n",
        "    try:\n",
        "        # 1. Download\n",
        "        r = requests.get(link, stream=True, timeout=30)\n",
        "        with open(temp_dbc, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        # 2. Descompressão\n",
        "        decompress(temp_dbc, temp_dbf)\n",
        "\n",
        "        # 3. Leitura e Pandas\n",
        "        table = DBF(temp_dbf, encoding='iso-8859-1', load=True)\n",
        "        df = pd.DataFrame(iter(table))\n",
        "\n",
        "        if not df.empty:\n",
        "            # Verifica colunas\n",
        "            if all(col in df.columns for col in colunas_desejadas):\n",
        "                df = df[colunas_desejadas].copy()\n",
        "                df['NR_BENEF_T'] = pd.to_numeric(df['NR_BENEF_T'], errors='coerce').fillna(0)\n",
        "\n",
        "                # Agrupa e Soma (Reduzindo drasticamente o tamanho dos dados antes de retornar)\n",
        "                df_agrupado = df.groupby(['ID_CMPT', 'CD_OPERADO'], as_index=False)['NR_BENEF_T'].sum()\n",
        "\n",
        "                # Cria chave Trimestre\n",
        "                df_agrupado['ID_TRIMESTRE'] = df_agrupado['ID_CMPT'].apply(_gerar_chave_trimestre)\n",
        "\n",
        "                resultado_df = df_agrupado\n",
        "            else:\n",
        "                print(f\"   [Worker] Ignorado {nome_arquivo}: Colunas ausentes.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   [Worker] Erro em {nome_arquivo}: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Limpeza rigorosa dos arquivos temporários deste processo\n",
        "        if os.path.exists(temp_dbc): os.remove(temp_dbc)\n",
        "        if os.path.exists(temp_dbf): os.remove(temp_dbf)\n",
        "\n",
        "    return resultado_df\n",
        "\n",
        "# --- CLASSE PRINCIPAL ---\n",
        "\n",
        "class ImportadorANSParalelo:\n",
        "    def __init__(self, db_path='dados_ans.db'):\n",
        "        self.db_path = db_path\n",
        "\n",
        "    def etapa_1_e_2_obter_links(self, url_origem):\n",
        "        print(f\"--- Mapeando arquivos em: {url_origem} ---\")\n",
        "        try:\n",
        "            response = requests.get(url_origem)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            links = []\n",
        "            for link in soup.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href and href.lower().endswith('.dbc'):\n",
        "                    links.append(urljoin(url_origem, href))\n",
        "            print(f\"Total de arquivos encontrados: {len(links)}\")\n",
        "            return links\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao obter links: {e}\")\n",
        "            return []\n",
        "\n",
        "    def etapa_3_processar_paralelo(self, lista_links, tabela_destino='beneficiarios_agrupados', max_workers=4):\n",
        "        \"\"\"\n",
        "        Gerencia os workers e grava no banco sequencialmente.\n",
        "        \"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        total = len(lista_links)\n",
        "        processados = 0\n",
        "\n",
        "        print(f\"--- Iniciando Processamento Paralelo ({max_workers} Workers) ---\")\n",
        "\n",
        "        # Inicia o Pool de Processos\n",
        "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submete todas as tarefas\n",
        "            # future_to_link é um dicionário para rastrear qual link pertence a qual tarefa\n",
        "            future_to_link = {executor.submit(processar_arquivo_worker, link): link for link in lista_links}\n",
        "\n",
        "            for future in as_completed(future_to_link):\n",
        "                processados += 1\n",
        "                link = future_to_link[future]\n",
        "                nome = link.split('/')[-1]\n",
        "\n",
        "                try:\n",
        "                    df_resultado = future.result()\n",
        "\n",
        "                    if df_resultado is not None and not df_resultado.empty:\n",
        "                        # O momento da escrita no banco é sequencial (Thread Principal)\n",
        "                        df_resultado.to_sql(tabela_destino, conn, if_exists='append', index=False)\n",
        "                        print(f\"[{processados}/{total}] Salvo: {nome} ({len(df_resultado)} registros)\")\n",
        "                    else:\n",
        "                        print(f\"[{processados}/{total}] Vazio/Ignorado: {nome}\")\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f\"[{processados}/{total}] Falha ao recuperar resultado de {nome}: {exc}\")\n",
        "\n",
        "        conn.close()\n",
        "        print(\"--- Processo Paralelo Finalizado ---\")\n",
        "\n",
        "# --- EXECUÇÃO ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL da ANS\n",
        "    url_ans = \"https://dadosabertos.ans.gov.br/FTP/Base_de_dados/Microdados/dados_dbc/beneficiarios/operadoras/\"\n",
        "\n",
        "    # Define quantos núcleos do processador você quer usar\n",
        "    # Se seu PC for potente, pode aumentar. Geralmente 4 ou 8 é um bom número.\n",
        "    WORKERS = os.cpu_count() or 4\n",
        "\n",
        "    bot = ImportadorANSParalelo(db_path='base_ans_paralela.db')\n",
        "\n",
        "    links = bot.etapa_1_e_2_obter_links(url_ans)\n",
        "\n",
        "    if links:\n",
        "        # Executa em paralelo\n",
        "        bot.etapa_3_processar_paralelo(links, max_workers=WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OGHjnffP557"
      },
      "source": [
        "## Inicio Extração e Tratamento Demonstração Contabeis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_8Kwf4PPP5pw"
      },
      "outputs": [],
      "source": [
        "def _extrair_info_arquivo_worker(nome_arquivo, url_pasta_ano):\n",
        "    \"\"\"Lógica de Regex para descobrir Ano e Trimestre\"\"\"\n",
        "    try:\n",
        "        nome = nome_arquivo.lower()\n",
        "\n",
        "        # 1. Extrair ANO da URL da pasta (Mais confiável)\n",
        "        url_limpa = url_pasta_ano.rstrip('/')\n",
        "        ano_pasta = url_limpa[-4:]\n",
        "\n",
        "        if not ano_pasta.isdigit() or len(ano_pasta) != 4:\n",
        "            match_ano = re.search(r'(20\\d{2})', nome)\n",
        "            if match_ano:\n",
        "                ano_pasta = match_ano.group(1)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        # 2. Extrair TRIMESTRE\n",
        "        # Regex 1: \"1 trimestre\", \"1_trimestre\", \"1-trimestre\"\n",
        "        match_longo = re.search(r'([1-4])\\s*[-_]?\\s*(?:trimestre|tri)', nome)\n",
        "        if match_longo: return f\"{ano_pasta}-T{match_longo.group(1)}\"\n",
        "\n",
        "        # Regex 2: \"1t\", \"4t\"\n",
        "        match_curto = re.search(r'([1-4])t', nome)\n",
        "        if match_curto: return f\"{ano_pasta}-T{match_curto.group(1)}\"\n",
        "\n",
        "        # Regex 3: \"t1\"\n",
        "        match_inv = re.search(r't([1-4])', nome)\n",
        "        if match_inv: return f\"{ano_pasta}-T{match_inv.group(1)}\"\n",
        "\n",
        "        return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def processar_zip_worker(args):\n",
        "    \"\"\"\n",
        "    Função que roda em paralelo.\n",
        "    Recebe uma tupla: (link_do_zip, url_da_pasta_ano)\n",
        "    Retorna: DataFrame filtrado ou None\n",
        "    \"\"\"\n",
        "    link_zip, url_pasta_ano = args\n",
        "    nome_arquivo = link_zip.split('/')[-1]\n",
        "\n",
        "    # Identifica a chave temporal\n",
        "    chave_trimestre = _extrair_info_arquivo_worker(nome_arquivo, url_pasta_ano)\n",
        "\n",
        "    if not chave_trimestre:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # 1. Download (timeout aumentado para evitar quedas em arquivos grandes)\n",
        "        r = requests.get(link_zip, timeout=120)\n",
        "\n",
        "        # 2. Processamento em Memória\n",
        "        with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
        "            csvs = [n for n in z.namelist() if n.lower().endswith('.csv')]\n",
        "            if not csvs: return None\n",
        "\n",
        "            nome_csv = csvs[0]\n",
        "            with z.open(nome_csv) as f:\n",
        "                # Lê tudo como string para não perder zeros a esquerda\n",
        "                df = pd.read_csv(f, sep=';', encoding='iso-8859-1', dtype=str)\n",
        "\n",
        "                # Limpa nomes das colunas (Upper + Strip)\n",
        "                df.columns = [c.upper().strip() for c in df.columns]\n",
        "\n",
        "                # Verifica se tem a coluna alvo\n",
        "                if 'CD_CONTA_CONTABIL' in df.columns:\n",
        "                    # FILTRA CONTA 31\n",
        "                    df_filtrado = df[df['CD_CONTA_CONTABIL'] == '31'].copy()\n",
        "\n",
        "                    if not df_filtrado.empty:\n",
        "                        # Adiciona a chave temporal\n",
        "                        df_filtrado['ID_TRIMESTRE'] = chave_trimestre\n",
        "\n",
        "                        # --- TRATAMENTO DE VALOR ---\n",
        "                        if 'VL_SALDO_FINAL' in df_filtrado.columns:\n",
        "                            df_filtrado['VL_SALDO_FINAL'] = df_filtrado['VL_SALDO_FINAL'].str.replace('.', '', regex=False)\n",
        "                            df_filtrado['VL_SALDO_FINAL'] = df_filtrado['VL_SALDO_FINAL'].str.replace(',', '.', regex=False)\n",
        "                            df_filtrado['VL_SALDO_FINAL'] = pd.to_numeric(df_filtrado['VL_SALDO_FINAL'], errors='coerce')\n",
        "\n",
        "                        # --- NOVO: SANITIZAÇÃO DE COLUNAS (CORREÇÃO DO ERRO) ---\n",
        "                        # Aqui definimos EXATAMENTE o que vai para o banco.\n",
        "                        # Qualquer coluna extra (como VL_SALDO_INICIAL ou DT_CARGA) será ignorada.\n",
        "                        colunas_finais = ['REG_ANS', 'CD_CONTA_CONTABIL', 'VL_SALDO_FINAL', 'ID_TRIMESTRE']\n",
        "\n",
        "                        # Verifica quais dessas colunas existem no DF atual (para evitar erro se faltar alguma)\n",
        "                        colunas_existentes = [c for c in colunas_finais if c in df_filtrado.columns]\n",
        "\n",
        "                        # Retorna apenas as colunas limpas\n",
        "                        return df_filtrado[colunas_existentes]\n",
        "\n",
        "    except Exception as e:\n",
        "        # Imprime erro mas não para o processo inteiro\n",
        "        # print(f\"   [Erro Worker] Falha em {nome_arquivo}: {e}\")\n",
        "        pass\n",
        "\n",
        "    return None\n",
        "\n",
        "# --- CLASSE PRINCIPAL ---\n",
        "\n",
        "class ExtratorContabilParalelo:\n",
        "    def __init__(self, db_path='dados_ans.db'):\n",
        "        self.db_path = db_path\n",
        "        self.url_base = \"https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/\"\n",
        "\n",
        "    def _mapear_todos_arquivos(self):\n",
        "        \"\"\"Varre as pastas de anos e retorna uma lista de tuplas (url_zip, url_ano)\"\"\"\n",
        "        print(f\"--- Mapeando estrutura de pastas em: {self.url_base} ---\")\n",
        "        tarefas = []\n",
        "\n",
        "        try:\n",
        "            # 1. Pega pastas de Anos\n",
        "            r = requests.get(self.url_base)\n",
        "            soup = BeautifulSoup(r.content, 'html.parser')\n",
        "            links_anos = []\n",
        "            for link in soup.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href and re.match(r'\\d{4}/', href):\n",
        "                    links_anos.append(urljoin(self.url_base, href))\n",
        "\n",
        "            print(f\"Anos encontrados: {len(links_anos)}. Buscando ZIPs dentro de cada ano...\")\n",
        "\n",
        "            # 2. Pega ZIPs dentro de cada Ano\n",
        "            # (Poderíamos paralelizar isso também, mas é rápido o suficiente ser sequencial)\n",
        "            for url_ano in links_anos:\n",
        "                try:\n",
        "                    r_ano = requests.get(url_ano)\n",
        "                    soup_ano = BeautifulSoup(r_ano.content, 'html.parser')\n",
        "                    for link in soup_ano.find_all('a'):\n",
        "                        href = link.get('href')\n",
        "                        if href and href.lower().endswith('.zip'):\n",
        "                            full_link = urljoin(url_ano, href)\n",
        "                            # Guardamos a tupla (Link do Arquivo, Link da Pasta do Ano)\n",
        "                            tarefas.append((full_link, url_ano))\n",
        "                except:\n",
        "                    print(f\"Erro ao ler pasta: {url_ano}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro no mapeamento: {e}\")\n",
        "\n",
        "        return tarefas\n",
        "\n",
        "    def executar(self, tabela_destino='demonstracoes_contabeis', max_workers=8):\n",
        "        # 1. Mapeamento (Sequencial, mas rápido)\n",
        "        lista_tarefas = self._mapear_todos_arquivos()\n",
        "        total_arquivos = len(lista_tarefas)\n",
        "\n",
        "        if total_arquivos == 0:\n",
        "            print(\"Nenhum arquivo encontrado.\")\n",
        "            return\n",
        "\n",
        "        print(f\"--- Iniciando Download e Processamento de {total_arquivos} arquivos ---\")\n",
        "        print(f\"--- Workers Ativos: {max_workers} ---\")\n",
        "\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        processados = 0\n",
        "        sucessos = 0\n",
        "\n",
        "        # 2. Processamento Paralelo\n",
        "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Envia todas as tarefas\n",
        "            future_to_url = {executor.submit(processar_zip_worker, tarefa): tarefa for tarefa in lista_tarefas}\n",
        "\n",
        "            for future in as_completed(future_to_url):\n",
        "                processados += 1\n",
        "                link, _ = future_to_url[future]\n",
        "                nome = link.split('/')[-1]\n",
        "\n",
        "                try:\n",
        "                    df_resultado = future.result()\n",
        "\n",
        "                    if df_resultado is not None and not df_resultado.empty:\n",
        "                        # 3. Escrita no Banco (Sequencial e Segura)\n",
        "                        df_resultado.to_sql(tabela_destino, conn, if_exists='append', index=False)\n",
        "                        sucessos += 1\n",
        "                        trimestre = df_resultado['ID_TRIMESTRE'].iloc[0]\n",
        "                        print(f\"[{processados}/{total_arquivos}] SALVO: {trimestre} ({len(df_resultado)} linhas) -> {nome}\")\n",
        "                    else:\n",
        "                        print(f\"[{processados}/{total_arquivos}] Ignorado/Vazio: {nome}\")\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f\"[{processados}/{total_arquivos}] Falha na tarefa {nome}: {exc}\")\n",
        "\n",
        "        conn.close()\n",
        "        print(f\"--- FIM. {sucessos} arquivos processados com sucesso. ---\")\n",
        "\n",
        "# --- EXECUÇÃO ---\n",
        "if __name__ == \"__main__\":\n",
        "    db_nome = 'base_ans_paralela.db'\n",
        "\n",
        "    # Ajuste o número de workers conforme sua internet e CPU\n",
        "    # 8 costuma ser um bom número. Se a internet cair, reduza para 4.\n",
        "    extrator = ExtratorContabilParalelo(db_path=db_nome)\n",
        "    extrator.executar(max_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFUUIl2d3WT"
      },
      "source": [
        "## Inicio Extração Dimensão Operadora\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0KXwXwhd9nB"
      },
      "outputs": [],
      "source": [
        "class ImportadorCadop:\n",
        "    def __init__(self, db_path, csv_path):\n",
        "        self.db_path = db_path\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "    def _limpar_nome_coluna(self, texto):\n",
        "        \"\"\"Limpa apenas o cabeçalho (nome da coluna) para o banco de dados\"\"\"\n",
        "        txt = unicodedata.normalize('NFKD', str(texto)).encode('ASCII', 'ignore').decode('ASCII')\n",
        "        return txt.strip().lower().replace(' ', '_').replace('.', '').replace('/', '')\n",
        "\n",
        "    def processar_cadop(self, tabela_destino='dim_operadoras'):\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            print(f\"ERRO: Arquivo não encontrado: {self.csv_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"--- Atualizando Tabela CADOP (Correção de Acentos) ---\")\n",
        "\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "\n",
        "            # 1. MUDANÇA CRÍTICA: encoding='utf-8-sig'\n",
        "            # 'utf-8' resolve o problema do \"SÃ£o\".\n",
        "            # O sufixo '-sig' é importante caso o arquivo tenha sido salvo pelo Excel (remove caracteres ocultos no início).\n",
        "            # dtype=str: Lemos TUDO como texto para proteger CPNJ e Código ANS (zeros a esquerda).\n",
        "            df = pd.read_csv(\n",
        "                self.csv_path,\n",
        "                sep=';',\n",
        "                encoding='utf-8-sig',\n",
        "                dtype=str\n",
        "            )\n",
        "\n",
        "            # 2. Padronizar nomes das colunas\n",
        "            df.columns = [self._limpar_nome_coluna(c) for c in df.columns]\n",
        "\n",
        "            # 3. TRATAMENTO INTELIGENTE DE COLUNAS\n",
        "            # O usuário pediu para verificar string vs int.\n",
        "            # Como carregamos tudo como 'str' para segurança, iteramos para limpar o texto.\n",
        "\n",
        "            print(\"   Aplicando tratamento nas colunas de texto...\")\n",
        "            for coluna in df.columns:\n",
        "                # Verifica se a coluna é do tipo objeto (string/texto)\n",
        "                if df[coluna].dtype == 'object':\n",
        "                    # .str.strip() remove espaços vazios no começo e fim que atrapalham SQL\n",
        "                    # Ex: \"São Paulo \" vira \"São Paulo\"\n",
        "                    df[coluna] = df[coluna].str.strip()\n",
        "\n",
        "            print(f\"   Colunas processadas: {list(df.columns[:5])} ...\")\n",
        "\n",
        "            # 4. Salvar no SQLite (DROP e CREATE com if_exists='replace')\n",
        "            df.to_sql(tabela_destino, conn, if_exists='replace', index=False)\n",
        "\n",
        "            # 5. Recriar Índice\n",
        "            if 'registro_ans' in df.columns:\n",
        "                cursor = conn.cursor()\n",
        "                cursor.execute(f'CREATE INDEX IF NOT EXISTS idx_cadop_reg ON {tabela_destino}(registro_ans);')\n",
        "                conn.commit()\n",
        "\n",
        "            conn.close()\n",
        "\n",
        "            # Validação visual\n",
        "            exemplo_cidade = df['bairro'].iloc[0] if 'bairro' in df.columns else 'Coluna não achada'\n",
        "            exemplo_razao = df['razao_social'].iloc[0]\n",
        "            print(f\"   -> Sucesso! Tabela atualizada.\")\n",
        "            print(f\"   -> Teste de Acento: '{exemplo_razao}'\")\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            print(\"   -> ERRO DE ENCODING: O arquivo não é UTF-8. Tente trocar para 'latin-1' no código.\")\n",
        "        except Exception as e:\n",
        "            print(f\"   -> Erro ao processar: {e}\")\n",
        "\n",
        "# --- EXECUÇÃO ---\n",
        "if __name__ == \"__main__\":\n",
        "    db_nome = 'base_ans_paralela.db'\n",
        "    arquivo_csv = 'Relatorio_cadop.csv'\n",
        "\n",
        "    if os.path.exists(arquivo_csv):\n",
        "        importador = ImportadorCadop(db_path=db_nome, csv_path=arquivo_csv)\n",
        "        importador.processar_cadop()\n",
        "    else:\n",
        "        print(f\"Arquivo {arquivo_csv} não encontrado.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNWZI/Is5Kim3d0kTQQtFHX",
      "include_colab_link": true,
      "mount_file_id": "1Oa7cHiEpspIkovnW_1S2e5TRd3w7Mj7q",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
