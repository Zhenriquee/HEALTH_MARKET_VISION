{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1Oa7cHiEpspIkovnW_1S2e5TRd3w7Mj7q",
      "authorship_tag": "ABX9TyPgLt3r2+0qQyBTXaWozBBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhenriquee/ANALISE_OPERADORAS/blob/main/Extra%C3%A7%C3%A3o_e_Tratamento_dos_Dados_ANS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coleta e Tratamento de Dados da ANS"
      ],
      "metadata": {
        "id": "6cxjhRlYP6JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivo:\n",
        "Realizar a extração dos dados da ANS com a finalidade de analisar o posicionamento da Unimed Caruaru com Relação as outras Operadoras, no final iremos armazenar esses dados em um arquivo .db e Utilizar o streamlit para projeção desses dados."
      ],
      "metadata": {
        "id": "rCC815l2RBWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Links Utilizados para Extração\n",
        "\n",
        "\n",
        "*   [Qtd. Beneficiarios por Trimestre](https://dadosabertos.ans.gov.br/FTP/Base_de_dados/Microdados/dados_dbc/beneficiarios/operadoras/)\n",
        "*   [Demonstração Contabeis](https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/)\n",
        "\n"
      ],
      "metadata": {
        "id": "RvLj1QOxP6Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bibliotecas Utilizadas"
      ],
      "metadata": {
        "id": "83gT3xLJYs1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import uuid\n",
        "import re\n",
        "import io\n",
        "import zipfile\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from datasus_dbc import decompress\n",
        "from dbfread import DBF\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed"
      ],
      "metadata": {
        "id": "1qo5p9oOYwF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio Extração e Tratamento Qtd. Beneficiarios por Trimestre\n",
        "\n"
      ],
      "metadata": {
        "id": "GD1UCLoJP6EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNÇÕES AUXILIARES (Worker) ---\n",
        "# Precisam estar fora da classe para o multiprocessing funcionar bem no Windows\n",
        "\n",
        "def _gerar_chave_trimestre(id_cmpt):\n",
        "    try:\n",
        "        s_cmpt = str(id_cmpt).strip()\n",
        "        if len(s_cmpt) < 6: return None\n",
        "        ano = s_cmpt[:4]\n",
        "        mes = int(s_cmpt[4:6])\n",
        "        trimestre = (mes - 1) // 3 + 1\n",
        "        return f\"{ano}-T{trimestre}\"\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def processar_arquivo_worker(link):\n",
        "    \"\"\"\n",
        "    Função isolada que roda em um núcleo separado da CPU.\n",
        "    Baixa, Converte, Filtra e Agrupa. Retorna um DataFrame pronto (ou None).\n",
        "    \"\"\"\n",
        "    nome_arquivo = link.split('/')[-1]\n",
        "\n",
        "    # Gera nomes únicos para evitar colisão entre processos\n",
        "    id_unico = str(uuid.uuid4())\n",
        "    temp_dbc = f\"temp_{id_unico}.dbc\"\n",
        "    temp_dbf = f\"temp_{id_unico}.dbf\"\n",
        "\n",
        "    colunas_desejadas = ['ID_CMPT', 'CD_OPERADO', 'NR_BENEF_T']\n",
        "    resultado_df = None\n",
        "\n",
        "    try:\n",
        "        # 1. Download\n",
        "        r = requests.get(link, stream=True, timeout=30)\n",
        "        with open(temp_dbc, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        # 2. Descompressão\n",
        "        decompress(temp_dbc, temp_dbf)\n",
        "\n",
        "        # 3. Leitura e Pandas\n",
        "        table = DBF(temp_dbf, encoding='iso-8859-1', load=True)\n",
        "        df = pd.DataFrame(iter(table))\n",
        "\n",
        "        if not df.empty:\n",
        "            # Verifica colunas\n",
        "            if all(col in df.columns for col in colunas_desejadas):\n",
        "                df = df[colunas_desejadas].copy()\n",
        "                df['NR_BENEF_T'] = pd.to_numeric(df['NR_BENEF_T'], errors='coerce').fillna(0)\n",
        "\n",
        "                # Agrupa e Soma (Reduzindo drasticamente o tamanho dos dados antes de retornar)\n",
        "                df_agrupado = df.groupby(['ID_CMPT', 'CD_OPERADO'], as_index=False)['NR_BENEF_T'].sum()\n",
        "\n",
        "                # Cria chave Trimestre\n",
        "                df_agrupado['ID_TRIMESTRE'] = df_agrupado['ID_CMPT'].apply(_gerar_chave_trimestre)\n",
        "\n",
        "                resultado_df = df_agrupado\n",
        "            else:\n",
        "                print(f\"   [Worker] Ignorado {nome_arquivo}: Colunas ausentes.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   [Worker] Erro em {nome_arquivo}: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Limpeza rigorosa dos arquivos temporários deste processo\n",
        "        if os.path.exists(temp_dbc): os.remove(temp_dbc)\n",
        "        if os.path.exists(temp_dbf): os.remove(temp_dbf)\n",
        "\n",
        "    return resultado_df\n",
        "\n",
        "# --- CLASSE PRINCIPAL ---\n",
        "\n",
        "class ImportadorANSParalelo:\n",
        "    def __init__(self, db_path='dados_ans.db'):\n",
        "        self.db_path = db_path\n",
        "\n",
        "    def etapa_1_e_2_obter_links(self, url_origem):\n",
        "        print(f\"--- Mapeando arquivos em: {url_origem} ---\")\n",
        "        try:\n",
        "            response = requests.get(url_origem)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            links = []\n",
        "            for link in soup.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href and href.lower().endswith('.dbc'):\n",
        "                    links.append(urljoin(url_origem, href))\n",
        "            print(f\"Total de arquivos encontrados: {len(links)}\")\n",
        "            return links\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao obter links: {e}\")\n",
        "            return []\n",
        "\n",
        "    def etapa_3_processar_paralelo(self, lista_links, tabela_destino='beneficiarios_agrupados', max_workers=4):\n",
        "        \"\"\"\n",
        "        Gerencia os workers e grava no banco sequencialmente.\n",
        "        \"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        total = len(lista_links)\n",
        "        processados = 0\n",
        "\n",
        "        print(f\"--- Iniciando Processamento Paralelo ({max_workers} Workers) ---\")\n",
        "\n",
        "        # Inicia o Pool de Processos\n",
        "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submete todas as tarefas\n",
        "            # future_to_link é um dicionário para rastrear qual link pertence a qual tarefa\n",
        "            future_to_link = {executor.submit(processar_arquivo_worker, link): link for link in lista_links}\n",
        "\n",
        "            for future in as_completed(future_to_link):\n",
        "                processados += 1\n",
        "                link = future_to_link[future]\n",
        "                nome = link.split('/')[-1]\n",
        "\n",
        "                try:\n",
        "                    df_resultado = future.result()\n",
        "\n",
        "                    if df_resultado is not None and not df_resultado.empty:\n",
        "                        # O momento da escrita no banco é sequencial (Thread Principal)\n",
        "                        df_resultado.to_sql(tabela_destino, conn, if_exists='append', index=False)\n",
        "                        print(f\"[{processados}/{total}] Salvo: {nome} ({len(df_resultado)} registros)\")\n",
        "                    else:\n",
        "                        print(f\"[{processados}/{total}] Vazio/Ignorado: {nome}\")\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f\"[{processados}/{total}] Falha ao recuperar resultado de {nome}: {exc}\")\n",
        "\n",
        "        conn.close()\n",
        "        print(\"--- Processo Paralelo Finalizado ---\")\n",
        "\n",
        "# --- EXECUÇÃO ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL da ANS\n",
        "    url_ans = \"https://dadosabertos.ans.gov.br/FTP/Base_de_dados/Microdados/dados_dbc/beneficiarios/operadoras/\"\n",
        "\n",
        "    # Define quantos núcleos do processador você quer usar\n",
        "    # Se seu PC for potente, pode aumentar. Geralmente 4 ou 8 é um bom número.\n",
        "    WORKERS = os.cpu_count() or 4\n",
        "\n",
        "    bot = ImportadorANSParalelo(db_path='base_ans_paralela.db')\n",
        "\n",
        "    links = bot.etapa_1_e_2_obter_links(url_ans)\n",
        "\n",
        "    if links:\n",
        "        # Executa em paralelo\n",
        "        bot.etapa_3_processar_paralelo(links, max_workers=WORKERS)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-hknZKxLRcmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio Extração e Tratamento Demonstração Contabeis"
      ],
      "metadata": {
        "id": "6OGHjnffP557"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtratorContabil:\n",
        "    def __init__(self, db_path='dados_ans.db'):\n",
        "        self.db_path = db_path\n",
        "        self.url_base = \"https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/\"\n",
        "\n",
        "    def _obter_links_anos(self):\n",
        "        \"\"\"Busca as pastas de anos (ex: 2022/, 2023/)\"\"\"\n",
        "        print(f\"--- Buscando pastas de anos em: {self.url_base} ---\")\n",
        "        try:\n",
        "            response = requests.get(self.url_base)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            links_anos = []\n",
        "            for link in soup.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                # Procura por pastas que pareçam anos (4 digitos + /)\n",
        "                if href and re.match(r'\\d{4}/', href):\n",
        "                    full_link = urljoin(self.url_base, href)\n",
        "                    links_anos.append(full_link)\n",
        "\n",
        "            return links_anos\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao listar anos: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _obter_zips_do_ano(self, url_ano):\n",
        "        \"\"\"Dentro da pasta do ano, busca os arquivos .zip\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url_ano)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            links_zips = []\n",
        "            for link in soup.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href and href.lower().endswith('.zip'):\n",
        "                    full_link = urljoin(url_ano, href)\n",
        "                    links_zips.append(full_link)\n",
        "            return links_zips\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao listar zips de {url_ano}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _extrair_trimestre_do_nome(self, nome_arquivo):\n",
        "        \"\"\"\n",
        "        Analisa o nome '1T2024.zip' ou '2024_1T.zip' e retorna '2024-T1'\n",
        "        \"\"\"\n",
        "        # Regex para capturar Trimestre (1 a 4) e Ano (20xx)\n",
        "        # Padrão comum na ANS: 1T2022, 1t2022, etc.\n",
        "        match = re.search(r'([1-4])t(20\\d{2})', nome_arquivo.lower())\n",
        "\n",
        "        if match:\n",
        "            trimestre = match.group(1)\n",
        "            ano = match.group(2)\n",
        "            return f\"{ano}-T{trimestre}\"\n",
        "\n",
        "        # Tentativa de padrão inverso (caso exista 2022_1t)\n",
        "        match_inv = re.search(r'(20\\d{2}).*([1-4])t', nome_arquivo.lower())\n",
        "        if match_inv:\n",
        "            ano = match_inv.group(1)\n",
        "            trimestre = match_inv.group(2)\n",
        "            return f\"{ano}-T{trimestre}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def executar_extracao(self, tabela_destino='demonstracoes_contabeis'):\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "\n",
        "        pastas_anos = self._obter_links_anos()\n",
        "\n",
        "        for pasta in pastas_anos:\n",
        "            print(f\"> Entrando na pasta: {pasta}\")\n",
        "            links_zips = self._obter_zips_do_ano(pasta)\n",
        "\n",
        "            for link_zip in links_zips:\n",
        "                nome_arquivo = link_zip.split('/')[-1]\n",
        "                chave_trimestre = self._extrair_trimestre_do_nome(nome_arquivo)\n",
        "\n",
        "                if not chave_trimestre:\n",
        "                    print(f\"   [Pular] Não foi possível identificar trimestre no nome: {nome_arquivo}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"   Processing: {nome_arquivo} -> Trimestre: {chave_trimestre}\")\n",
        "\n",
        "                try:\n",
        "                    # 1. Download em Memória (Stream)\n",
        "                    r = requests.get(link_zip)\n",
        "\n",
        "                    # 2. Abrir ZIP da memória (io.BytesIO)\n",
        "                    with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
        "                        # Procura o CSV dentro do zip\n",
        "                        csvs = [n for n in z.namelist() if n.lower().endswith('.csv')]\n",
        "\n",
        "                        if not csvs:\n",
        "                            print(\"   [Erro] Nenhum CSV encontrado dentro do zip.\")\n",
        "                            continue\n",
        "\n",
        "                        # Vamos assumir que o primeiro CSV é o correto\n",
        "                        nome_csv = csvs[0]\n",
        "\n",
        "                        # 3. Ler CSV direto do ZIP\n",
        "                        with z.open(nome_csv) as f:\n",
        "                            # ANS costuma usar separador ';' e encoding 'latin1' ou 'utf-8'\n",
        "                            # 'CD_CONTA_CONTABIL' as vezes vem como string ou int, vamos forçar conversão depois\n",
        "                            df = pd.read_csv(f, sep=';', encoding='iso-8859-1', dtype=str)\n",
        "\n",
        "                            # Tratamento de colunas (Minúsculo para padronizar busca)\n",
        "                            df.columns = [c.upper() for c in df.columns]\n",
        "\n",
        "                            if 'CD_CONTA_CONTABIL' in df.columns:\n",
        "                                # 4. Filtrar CD_CONTA_CONTABIL == 31\n",
        "                                # Convertemos para string para garantir a comparação exata\n",
        "                                df_filtrado = df[df['CD_CONTA_CONTABIL'] == '31'].copy()\n",
        "\n",
        "                                if not df_filtrado.empty:\n",
        "                                    # 5. Adicionar coluna de Trimestre\n",
        "                                    df_filtrado['ID_TRIMESTRE'] = chave_trimestre\n",
        "\n",
        "                                    # Conversão de tipos úteis (Ex: VL_SALDO_FINAL para float)\n",
        "                                    # A ANS usa vírgula como decimal no CSV pt-br\n",
        "                                    if 'VL_SALDO_FINAL' in df_filtrado.columns:\n",
        "                                        df_filtrado['VL_SALDO_FINAL'] = df_filtrado['VL_SALDO_FINAL'].str.replace(',', '.', regex=False)\n",
        "                                        df_filtrado['VL_SALDO_FINAL'] = pd.to_numeric(df_filtrado['VL_SALDO_FINAL'], errors='coerce')\n",
        "\n",
        "                                    # 6. Salvar no Banco\n",
        "                                    df_filtrado.to_sql(tabela_destino, conn, if_exists='append', index=False)\n",
        "                                    print(f\"      -> Sucesso! {len(df_filtrado)} registros conta 31 salvos.\")\n",
        "                                else:\n",
        "                                    print(\"      -> Arquivo lido, mas sem registros da conta 31.\")\n",
        "                            else:\n",
        "                                print(f\"      -> Coluna CD_CONTA_CONTABIL não encontrada. Colunas: {list(df.columns)}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"      -> Erro ao processar zip: {e}\")\n",
        "\n",
        "        conn.close()\n",
        "        print(\"--- Extração Contábil Finalizada ---\")\n",
        "\n",
        "# --- EXECUÇÃO ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Importante: Use o MESMO nome de banco que você usou no script anterior\n",
        "    # para que as tabelas fiquem juntas.\n",
        "    # Exemplo anterior: 'base_ans_paralela.db' ou 'base_ans_v2.db'\n",
        "\n",
        "    db_nome = 'base_ans_paralela.db'\n",
        "\n",
        "    print(f\"Iniciando extração contábil para o banco: {db_nome}\")\n",
        "    extrator = ExtratorContabil(db_path=db_nome)\n",
        "    extrator.executar_extracao()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_8Kwf4PPP5pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio Extração Dimensão Operadora\n"
      ],
      "metadata": {
        "id": "EOFUUIl2d3WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImportadorCadop:\n",
        "    def __init__(self, db_path, csv_path):\n",
        "        self.db_path = db_path\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "    def _limpar_nome_coluna(self, texto):\n",
        "        \"\"\"Limpa apenas o cabeçalho (nome da coluna) para o banco de dados\"\"\"\n",
        "        txt = unicodedata.normalize('NFKD', str(texto)).encode('ASCII', 'ignore').decode('ASCII')\n",
        "        return txt.strip().lower().replace(' ', '_').replace('.', '').replace('/', '')\n",
        "\n",
        "    def processar_cadop(self, tabela_destino='dim_operadoras'):\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            print(f\"ERRO: Arquivo não encontrado: {self.csv_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"--- Atualizando Tabela CADOP (Correção de Acentos) ---\")\n",
        "\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "\n",
        "            # 1. MUDANÇA CRÍTICA: encoding='utf-8-sig'\n",
        "            # 'utf-8' resolve o problema do \"SÃ£o\".\n",
        "            # O sufixo '-sig' é importante caso o arquivo tenha sido salvo pelo Excel (remove caracteres ocultos no início).\n",
        "            # dtype=str: Lemos TUDO como texto para proteger CPNJ e Código ANS (zeros a esquerda).\n",
        "            df = pd.read_csv(\n",
        "                self.csv_path,\n",
        "                sep=';',\n",
        "                encoding='utf-8-sig',\n",
        "                dtype=str\n",
        "            )\n",
        "\n",
        "            # 2. Padronizar nomes das colunas\n",
        "            df.columns = [self._limpar_nome_coluna(c) for c in df.columns]\n",
        "\n",
        "            # 3. TRATAMENTO INTELIGENTE DE COLUNAS\n",
        "            # O usuário pediu para verificar string vs int.\n",
        "            # Como carregamos tudo como 'str' para segurança, iteramos para limpar o texto.\n",
        "\n",
        "            print(\"   Aplicando tratamento nas colunas de texto...\")\n",
        "            for coluna in df.columns:\n",
        "                # Verifica se a coluna é do tipo objeto (string/texto)\n",
        "                if df[coluna].dtype == 'object':\n",
        "                    # .str.strip() remove espaços vazios no começo e fim que atrapalham SQL\n",
        "                    # Ex: \"São Paulo \" vira \"São Paulo\"\n",
        "                    df[coluna] = df[coluna].str.strip()\n",
        "\n",
        "            print(f\"   Colunas processadas: {list(df.columns[:5])} ...\")\n",
        "\n",
        "            # 4. Salvar no SQLite (DROP e CREATE com if_exists='replace')\n",
        "            df.to_sql(tabela_destino, conn, if_exists='replace', index=False)\n",
        "\n",
        "            # 5. Recriar Índice\n",
        "            if 'registro_ans' in df.columns:\n",
        "                cursor = conn.cursor()\n",
        "                cursor.execute(f'CREATE INDEX IF NOT EXISTS idx_cadop_reg ON {tabela_destino}(registro_ans);')\n",
        "                conn.commit()\n",
        "\n",
        "            conn.close()\n",
        "\n",
        "            # Validação visual\n",
        "            exemplo_cidade = df['bairro'].iloc[0] if 'bairro' in df.columns else 'Coluna não achada'\n",
        "            exemplo_razao = df['razao_social'].iloc[0]\n",
        "            print(f\"   -> Sucesso! Tabela atualizada.\")\n",
        "            print(f\"   -> Teste de Acento: '{exemplo_razao}'\")\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            print(\"   -> ERRO DE ENCODING: O arquivo não é UTF-8. Tente trocar para 'latin-1' no código.\")\n",
        "        except Exception as e:\n",
        "            print(f\"   -> Erro ao processar: {e}\")\n",
        "\n",
        "# --- EXECUÇÃO ---\n",
        "if __name__ == \"__main__\":\n",
        "    db_nome = 'base_ans_paralela.db'\n",
        "    arquivo_csv = 'Relatorio_cadop.csv'\n",
        "\n",
        "    if os.path.exists(arquivo_csv):\n",
        "        importador = ImportadorCadop(db_path=db_nome, csv_path=arquivo_csv)\n",
        "        importador.processar_cadop()\n",
        "    else:\n",
        "        print(f\"Arquivo {arquivo_csv} não encontrado.\")"
      ],
      "metadata": {
        "id": "-0KXwXwhd9nB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}