{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Oa7cHiEpspIkovnW_1S2e5TRd3w7Mj7q",
      "authorship_tag": "ABX9TyP8Nank7XlnTWyAi4hiyqaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhenriquee/HEALTH_MARKET_VISION/blob/main/Extra%C3%A7%C3%A3o_e_Tratamento_dos_Dados_ANS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coleta e Tratamento de Dados da ANS"
      ],
      "metadata": {
        "id": "6cxjhRlYP6JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivo:\n",
        "Realizar a extração dos dados da ANS com a finalidade de analisar o posicionamento da Unimed Caruaru com Relação as outras Operadoras, no final iremos armazenar esses dados em um arquivo .db e Utilizar o streamlit para projeção desses dados."
      ],
      "metadata": {
        "id": "rCC815l2RBWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Links Utilizados para Extração\n",
        "\n",
        "\n",
        "*   [Qtd. Beneficiarios por Trimestre](https://dadosabertos.ans.gov.br/FTP/Base_de_dados/Microdados/dados_dbc/beneficiarios/operadoras/)\n",
        "*   [Demonstração Contabeis](https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/)\n",
        "*   [Dimensão Operadora](https://dados-abertos-service.pr.ans.gov.br/swagger-ui/index.html?configUrl=/v3/api-docs/swagger-config#/)\n"
      ],
      "metadata": {
        "id": "RvLj1QOxP6Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bibliotecas Utilizadas"
      ],
      "metadata": {
        "id": "83gT3xLJYs1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasus-dbc dbfread requests curl_cffi bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3eXn5cwmFV-",
        "outputId": "d61787c8-c754-4884-cc87-0956ca965e8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasus-dbc\n",
            "  Downloading datasus_dbc-0.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting dbfread\n",
            "  Downloading dbfread-2.0.7-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: curl_cffi in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi) (2.0.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi) (2.23)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (4.15.0)\n",
            "Downloading datasus_dbc-0.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dbfread-2.0.7-py2.py3-none-any.whl (20 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: dbfread, datasus-dbc, bs4\n",
            "Successfully installed bs4-0.0.2 datasus-dbc-0.1.3 dbfread-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import uuid\n",
        "import re\n",
        "import io\n",
        "import zipfile\n",
        "import unicodedata\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from datasus_dbc import decompress\n",
        "from dbfread import DBF\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from curl_cffi import requests as requests_curl\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ],
      "metadata": {
        "id": "1qo5p9oOYwF2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio Extração e Tratamento Qtd. Beneficiarios por Trimestre\n",
        "\n"
      ],
      "metadata": {
        "id": "GD1UCLoJP6EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNÇÕES AUXILIARES (Worker) ---\n",
        "# Precisam estar fora da classe para o multiprocessing funcionar bem no Windows\n",
        "\n",
        "def _gerar_chave_trimestre(id_cmpt):\n",
        "    try:\n",
        "        s_cmpt = str(id_cmpt).strip()\n",
        "        if len(s_cmpt) < 6: return None\n",
        "        ano = s_cmpt[:4]\n",
        "        mes = int(s_cmpt[4:6])\n",
        "        trimestre = (mes - 1) // 3 + 1\n",
        "        return f\"{ano}-T{trimestre}\"\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def processar_arquivo_worker(link):\n",
        "    \"\"\"\n",
        "    Função isolada que roda em um núcleo separado da CPU.\n",
        "    Baixa, Converte, Filtra e Agrupa. Retorna um DataFrame pronto (ou None).\n",
        "    \"\"\"\n",
        "    nome_arquivo = link.split('/')[-1]\n",
        "\n",
        "    # Gera nomes únicos para evitar colisão entre processos\n",
        "    id_unico = str(uuid.uuid4())\n",
        "    temp_dbc = f\"temp_{id_unico}.dbc\"\n",
        "    temp_dbf = f\"temp_{id_unico}.dbf\"\n",
        "\n",
        "    colunas_desejadas = ['ID_CMPT', 'CD_OPERADO', 'NR_BENEF_T']\n",
        "    resultado_df = None\n",
        "\n",
        "    try:\n",
        "        # 1. Download\n",
        "        r = requests.get(link, stream=True, timeout=30)\n",
        "        with open(temp_dbc, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        # 2. Descompressão\n",
        "        decompress(temp_dbc, temp_dbf)\n",
        "\n",
        "        # 3. Leitura e Pandas\n",
        "        table = DBF(temp_dbf, encoding='iso-8859-1', load=True)\n",
        "        df = pd.DataFrame(iter(table))\n",
        "\n",
        "        if not df.empty:\n",
        "            # Verifica colunas\n",
        "            if all(col in df.columns for col in colunas_desejadas):\n",
        "                df = df[colunas_desejadas].copy()\n",
        "                df['NR_BENEF_T'] = pd.to_numeric(df['NR_BENEF_T'], errors='coerce').fillna(0)\n",
        "\n",
        "                # Agrupa e Soma (Reduzindo drasticamente o tamanho dos dados antes de retornar)\n",
        "                df_agrupado = df.groupby(['ID_CMPT', 'CD_OPERADO'], as_index=False)['NR_BENEF_T'].sum()\n",
        "\n",
        "                # Cria chave Trimestre\n",
        "                df_agrupado['ID_TRIMESTRE'] = df_agrupado['ID_CMPT'].apply(_gerar_chave_trimestre)\n",
        "\n",
        "                resultado_df = df_agrupado\n",
        "            else:\n",
        "                print(f\"   [Worker] Ignorado {nome_arquivo}: Colunas ausentes.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   [Worker] Erro em {nome_arquivo}: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Limpeza rigorosa dos arquivos temporários deste processo\n",
        "        if os.path.exists(temp_dbc): os.remove(temp_dbc)\n",
        "        if os.path.exists(temp_dbf): os.remove(temp_dbf)\n",
        "\n",
        "    return resultado_df\n",
        "\n",
        "# --- CLASSE PRINCIPAL ---\n",
        "\n",
        "class ImportadorANSParalelo:\n",
        "    def __init__(self, db_path='dados_ans.db'):\n",
        "        self.db_path = db_path\n",
        "\n",
        "    def etapa_1_e_2_obter_links(self, url_origem):\n",
        "        print(f\"--- Mapeando arquivos em: {url_origem} ---\")\n",
        "        try:\n",
        "            response = requests.get(url_origem)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            links = []\n",
        "            for link in soup.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href and href.lower().endswith('.dbc'):\n",
        "                    links.append(urljoin(url_origem, href))\n",
        "            print(f\"Total de arquivos encontrados: {len(links)}\")\n",
        "            return links\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao obter links: {e}\")\n",
        "            return []\n",
        "\n",
        "    def etapa_3_processar_paralelo(self, lista_links, tabela_destino='beneficiarios_agrupados', max_workers=4):\n",
        "        \"\"\"\n",
        "        Gerencia os workers e grava no banco sequencialmente.\n",
        "        \"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        total = len(lista_links)\n",
        "        processados = 0\n",
        "\n",
        "        print(f\"--- Iniciando Processamento Paralelo ({max_workers} Workers) ---\")\n",
        "\n",
        "        # Inicia o Pool de Processos\n",
        "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submete todas as tarefas\n",
        "            # future_to_link é um dicionário para rastrear qual link pertence a qual tarefa\n",
        "            future_to_link = {executor.submit(processar_arquivo_worker, link): link for link in lista_links}\n",
        "\n",
        "            for future in as_completed(future_to_link):\n",
        "                processados += 1\n",
        "                link = future_to_link[future]\n",
        "                nome = link.split('/')[-1]\n",
        "\n",
        "                try:\n",
        "                    df_resultado = future.result()\n",
        "\n",
        "                    if df_resultado is not None and not df_resultado.empty:\n",
        "                        # O momento da escrita no banco é sequencial (Thread Principal)\n",
        "                        df_resultado.to_sql(tabela_destino, conn, if_exists='append', index=False)\n",
        "                        print(f\"[{processados}/{total}] Salvo: {nome} ({len(df_resultado)} registros)\")\n",
        "                    else:\n",
        "                        print(f\"[{processados}/{total}] Vazio/Ignorado: {nome}\")\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f\"[{processados}/{total}] Falha ao recuperar resultado de {nome}: {exc}\")\n",
        "\n",
        "        conn.close()\n",
        "        print(\"--- Processo Paralelo Finalizado ---\")\n",
        "\n",
        "# --- EXECUÇÃO ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL da ANS\n",
        "    url_ans = \"https://dadosabertos.ans.gov.br/FTP/Base_de_dados/Microdados/dados_dbc/beneficiarios/operadoras/\"\n",
        "\n",
        "    # Define quantos núcleos do processador você quer usar\n",
        "    # Se seu PC for potente, pode aumentar. Geralmente 4 ou 8 é um bom número.\n",
        "    WORKERS = os.cpu_count() or 4\n",
        "\n",
        "    bot = ImportadorANSParalelo(db_path='base_ans_paralela.db')\n",
        "\n",
        "    links = bot.etapa_1_e_2_obter_links(url_ans)\n",
        "\n",
        "    if links:\n",
        "        # Executa em paralelo\n",
        "        bot.etapa_3_processar_paralelo(links, max_workers=WORKERS)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-hknZKxLRcmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c24de04-dca3-4048-d1a7-ec40ce70106f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Mapeando arquivos em: https://dadosabertos.ans.gov.br/FTP/Base_de_dados/Microdados/dados_dbc/beneficiarios/operadoras/ ---\n",
            "Total de arquivos encontrados: 58\n",
            "--- Iniciando Processamento Paralelo (2 Workers) ---\n",
            "[1/58] Salvo: tb_cc_2011-06.dbc (1413 registros)\n",
            "[2/58] Salvo: tb_cc_2011-09.dbc (1405 registros)\n",
            "[3/58] Salvo: tb_cc_2012-03.dbc (1384 registros)\n",
            "[4/58] Salvo: tb_cc_2011-12.dbc (1391 registros)\n",
            "[5/58] Salvo: tb_cc_2012-06.dbc (1377 registros)\n",
            "[6/58] Salvo: tb_cc_2012-09.dbc (1356 registros)\n",
            "[7/58] Salvo: tb_cc_2012-12.dbc (1346 registros)\n",
            "[8/58] Salvo: tb_cc_2013-03.dbc (1330 registros)\n",
            "[9/58] Salvo: tb_cc_2013-06.dbc (1307 registros)\n",
            "[10/58] Salvo: tb_cc_2013-09.dbc (1295 registros)\n",
            "[11/58] Salvo: tb_cc_2013-12.dbc (1276 registros)\n",
            "[12/58] Salvo: tb_cc_2014-03.dbc (1271 registros)\n",
            "[13/58] Salvo: tb_cc_2014-06.dbc (1260 registros)\n",
            "[14/58] Salvo: tb_cc_2014-09.dbc (1242 registros)\n",
            "[15/58] Salvo: tb_cc_2015-03.dbc (1221 registros)\n",
            "[16/58] Salvo: tb_cc_2014-12.dbc (1243 registros)\n",
            "[17/58] Salvo: tb_cc_2015-09.dbc (1185 registros)\n",
            "[18/58] Salvo: tb_cc_2015-06.dbc (1210 registros)\n",
            "[19/58] Salvo: tb_cc_2015-12.dbc (1162 registros)\n",
            "[20/58] Salvo: tb_cc_2016-03.dbc (1143 registros)\n",
            "[21/58] Salvo: tb_cc_2016-06.dbc (1122 registros)\n",
            "[22/58] Salvo: tb_cc_2016-09.dbc (1112 registros)\n",
            "[23/58] Salvo: tb_cc_2016-12.dbc (1105 registros)\n",
            "[24/58] Salvo: tb_cc_2017-03.dbc (1090 registros)\n",
            "[25/58] Salvo: tb_cc_2017-09.dbc (1074 registros)\n",
            "[26/58] Salvo: tb_cc_2017-06.dbc (1082 registros)\n",
            "[27/58] Salvo: tb_cc_2018-03.dbc (1060 registros)\n",
            "[28/58] Salvo: tb_cc_2017-12.dbc (1061 registros)\n",
            "[29/58] Salvo: tb_cc_2018-06.dbc (1055 registros)\n",
            "[30/58] Salvo: tb_cc_2018-09.dbc (1055 registros)\n",
            "[31/58] Salvo: tb_cc_2018-12.dbc (1039 registros)\n",
            "[32/58] Salvo: tb_cc_2019-03.dbc (1026 registros)\n",
            "[33/58] Salvo: tb_cc_2019-06.dbc (1021 registros)\n",
            "[34/58] Salvo: tb_cc_2019-09.dbc (1014 registros)\n",
            "[35/58] Salvo: tb_cc_2019-12.dbc (1014 registros)\n",
            "[36/58] Salvo: tb_cc_2020-03.dbc (1010 registros)\n",
            "[37/58] Salvo: tb_cc_2020-06.dbc (992 registros)\n",
            "[38/58] Salvo: tb_cc_2020-09.dbc (984 registros)\n",
            "[39/58] Salvo: tb_cc_2020-12.dbc (972 registros)\n",
            "[40/58] Salvo: tb_cc_2021-03.dbc (966 registros)\n",
            "[41/58] Salvo: tb_cc_2021-06.dbc (965 registros)\n",
            "[42/58] Salvo: tb_cc_2021-09.dbc (961 registros)\n",
            "[43/58] Salvo: tb_cc_2021-12.dbc (949 registros)\n",
            "[44/58] Salvo: tb_cc_2022-03.dbc (949 registros)\n",
            "[45/58] Salvo: tb_cc_2022-06.dbc (943 registros)\n",
            "[46/58] Salvo: tb_cc_2022-09.dbc (936 registros)\n",
            "[47/58] Salvo: tb_cc_2022-12.dbc (925 registros)\n",
            "[48/58] Salvo: tb_cc_2023-03.dbc (921 registros)\n",
            "[49/58] Salvo: tb_cc_2023-06.dbc (917 registros)\n",
            "[50/58] Salvo: tb_cc_2023-09.dbc (917 registros)\n",
            "[51/58] Salvo: tb_cc_2023-12.dbc (917 registros)\n",
            "[52/58] Salvo: tb_cc_2024-03.dbc (917 registros)\n",
            "[53/58] Salvo: tb_cc_2024-06.dbc (908 registros)\n",
            "[54/58] Salvo: tb_cc_2024-09.dbc (909 registros)\n",
            "[55/58] Salvo: tb_cc_2024-12.dbc (908 registros)\n",
            "[56/58] Salvo: tb_cc_2025-03.dbc (904 registros)\n",
            "[57/58] Salvo: tb_cc_2025-06.dbc (899 registros)\n",
            "[58/58] Salvo: tb_cc_2025-09.dbc (894 registros)\n",
            "--- Processo Paralelo Finalizado ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio Extração e Tratamento Demonstração Contabeis"
      ],
      "metadata": {
        "id": "6OGHjnffP557"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _extrair_info_arquivo_worker(nome_arquivo, url_pasta_ano):\n",
        "    \"\"\"Lógica de Regex para descobrir Ano e Trimestre\"\"\"\n",
        "    try:\n",
        "        nome = nome_arquivo.lower()\n",
        "\n",
        "        # 1. Extrair ANO da URL da pasta (Mais confiável)\n",
        "        url_limpa = url_pasta_ano.rstrip('/')\n",
        "        ano_pasta = url_limpa[-4:]\n",
        "\n",
        "        if not ano_pasta.isdigit() or len(ano_pasta) != 4:\n",
        "            match_ano = re.search(r'(20\\d{2})', nome)\n",
        "            if match_ano:\n",
        "                ano_pasta = match_ano.group(1)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        # 2. Extrair TRIMESTRE\n",
        "        # Regex 1: \"1 trimestre\", \"1_trimestre\", \"1-trimestre\"\n",
        "        match_longo = re.search(r'([1-4])\\s*[-_]?\\s*(?:trimestre|tri)', nome)\n",
        "        if match_longo: return f\"{ano_pasta}-T{match_longo.group(1)}\"\n",
        "\n",
        "        # Regex 2: \"1t\", \"4t\"\n",
        "        match_curto = re.search(r'([1-4])t', nome)\n",
        "        if match_curto: return f\"{ano_pasta}-T{match_curto.group(1)}\"\n",
        "\n",
        "        # Regex 3: \"t1\"\n",
        "        match_inv = re.search(r't([1-4])', nome)\n",
        "        if match_inv: return f\"{ano_pasta}-T{match_inv.group(1)}\"\n",
        "\n",
        "        return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def processar_zip_worker(args):\n",
        "    \"\"\"\n",
        "    Função que roda em paralelo.\n",
        "    Recebe uma tupla: (link_do_zip, url_da_pasta_ano)\n",
        "    Retorna: DataFrame filtrado ou None\n",
        "    \"\"\"\n",
        "    link_zip, url_pasta_ano = args\n",
        "    nome_arquivo = link_zip.split('/')[-1]\n",
        "\n",
        "    # Identifica a chave temporal\n",
        "    chave_trimestre = _extrair_info_arquivo_worker(nome_arquivo, url_pasta_ano)\n",
        "\n",
        "    if not chave_trimestre:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # 1. Download (timeout aumentado para evitar quedas em arquivos grandes)\n",
        "        r = requests.get(link_zip, timeout=120)\n",
        "\n",
        "        # 2. Processamento em Memória\n",
        "        with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
        "            csvs = [n for n in z.namelist() if n.lower().endswith('.csv')]\n",
        "            if not csvs: return None\n",
        "\n",
        "            nome_csv = csvs[0]\n",
        "            with z.open(nome_csv) as f:\n",
        "                # Lê tudo como string para não perder zeros a esquerda\n",
        "                df = pd.read_csv(f, sep=';', encoding='iso-8859-1', dtype=str)\n",
        "\n",
        "                # Limpa nomes das colunas (Upper + Strip)\n",
        "                df.columns = [c.upper().strip() for c in df.columns]\n",
        "\n",
        "                # Verifica se tem a coluna alvo\n",
        "                if 'CD_CONTA_CONTABIL' in df.columns:\n",
        "                    # FILTRA CONTA 31\n",
        "                    df_filtrado = df[df['CD_CONTA_CONTABIL'] == '31'].copy()\n",
        "\n",
        "                    if not df_filtrado.empty:\n",
        "                        # Adiciona a chave temporal\n",
        "                        df_filtrado['ID_TRIMESTRE'] = chave_trimestre\n",
        "\n",
        "                        # --- TRATAMENTO DE VALOR ---\n",
        "                        if 'VL_SALDO_FINAL' in df_filtrado.columns:\n",
        "                            df_filtrado['VL_SALDO_FINAL'] = df_filtrado['VL_SALDO_FINAL'].str.replace('.', '', regex=False)\n",
        "                            df_filtrado['VL_SALDO_FINAL'] = df_filtrado['VL_SALDO_FINAL'].str.replace(',', '.', regex=False)\n",
        "                            df_filtrado['VL_SALDO_FINAL'] = pd.to_numeric(df_filtrado['VL_SALDO_FINAL'], errors='coerce')\n",
        "\n",
        "                        # --- NOVO: SANITIZAÇÃO DE COLUNAS (CORREÇÃO DO ERRO) ---\n",
        "                        # Aqui definimos EXATAMENTE o que vai para o banco.\n",
        "                        # Qualquer coluna extra (como VL_SALDO_INICIAL ou DT_CARGA) será ignorada.\n",
        "                        colunas_finais = ['REG_ANS', 'CD_CONTA_CONTABIL', 'VL_SALDO_FINAL', 'ID_TRIMESTRE']\n",
        "\n",
        "                        # Verifica quais dessas colunas existem no DF atual (para evitar erro se faltar alguma)\n",
        "                        colunas_existentes = [c for c in colunas_finais if c in df_filtrado.columns]\n",
        "\n",
        "                        # Retorna apenas as colunas limpas\n",
        "                        return df_filtrado[colunas_existentes]\n",
        "\n",
        "    except Exception as e:\n",
        "        # Imprime erro mas não para o processo inteiro\n",
        "        # print(f\"   [Erro Worker] Falha em {nome_arquivo}: {e}\")\n",
        "        pass\n",
        "\n",
        "    return None\n",
        "\n",
        "# --- CLASSE PRINCIPAL ---\n",
        "\n",
        "class ExtratorContabilParalelo:\n",
        "    def __init__(self, db_path='dados_ans.db'):\n",
        "        self.db_path = db_path\n",
        "        self.url_base = \"https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/\"\n",
        "\n",
        "    def _mapear_todos_arquivos(self):\n",
        "        \"\"\"Varre as pastas de anos e retorna uma lista de tuplas (url_zip, url_ano)\"\"\"\n",
        "        print(f\"--- Mapeando estrutura de pastas em: {self.url_base} ---\")\n",
        "        tarefas = []\n",
        "\n",
        "        try:\n",
        "            # 1. Pega pastas de Anos\n",
        "            r = requests.get(self.url_base)\n",
        "            soup = BeautifulSoup(r.content, 'html.parser')\n",
        "            links_anos = []\n",
        "            for link in soup.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href and re.match(r'\\d{4}/', href):\n",
        "                    links_anos.append(urljoin(self.url_base, href))\n",
        "\n",
        "            print(f\"Anos encontrados: {len(links_anos)}. Buscando ZIPs dentro de cada ano...\")\n",
        "\n",
        "            # 2. Pega ZIPs dentro de cada Ano\n",
        "            # (Poderíamos paralelizar isso também, mas é rápido o suficiente ser sequencial)\n",
        "            for url_ano in links_anos:\n",
        "                try:\n",
        "                    r_ano = requests.get(url_ano)\n",
        "                    soup_ano = BeautifulSoup(r_ano.content, 'html.parser')\n",
        "                    for link in soup_ano.find_all('a'):\n",
        "                        href = link.get('href')\n",
        "                        if href and href.lower().endswith('.zip'):\n",
        "                            full_link = urljoin(url_ano, href)\n",
        "                            # Guardamos a tupla (Link do Arquivo, Link da Pasta do Ano)\n",
        "                            tarefas.append((full_link, url_ano))\n",
        "                except:\n",
        "                    print(f\"Erro ao ler pasta: {url_ano}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro no mapeamento: {e}\")\n",
        "\n",
        "        return tarefas\n",
        "\n",
        "    def executar(self, tabela_destino='demonstracoes_contabeis', max_workers=8):\n",
        "        # 1. Mapeamento (Sequencial, mas rápido)\n",
        "        lista_tarefas = self._mapear_todos_arquivos()\n",
        "        total_arquivos = len(lista_tarefas)\n",
        "\n",
        "        if total_arquivos == 0:\n",
        "            print(\"Nenhum arquivo encontrado.\")\n",
        "            return\n",
        "\n",
        "        print(f\"--- Iniciando Download e Processamento de {total_arquivos} arquivos ---\")\n",
        "        print(f\"--- Workers Ativos: {max_workers} ---\")\n",
        "\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        processados = 0\n",
        "        sucessos = 0\n",
        "\n",
        "        # 2. Processamento Paralelo\n",
        "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Envia todas as tarefas\n",
        "            future_to_url = {executor.submit(processar_zip_worker, tarefa): tarefa for tarefa in lista_tarefas}\n",
        "\n",
        "            for future in as_completed(future_to_url):\n",
        "                processados += 1\n",
        "                link, _ = future_to_url[future]\n",
        "                nome = link.split('/')[-1]\n",
        "\n",
        "                try:\n",
        "                    df_resultado = future.result()\n",
        "\n",
        "                    if df_resultado is not None and not df_resultado.empty:\n",
        "                        # 3. Escrita no Banco (Sequencial e Segura)\n",
        "                        df_resultado.to_sql(tabela_destino, conn, if_exists='append', index=False)\n",
        "                        sucessos += 1\n",
        "                        trimestre = df_resultado['ID_TRIMESTRE'].iloc[0]\n",
        "                        print(f\"[{processados}/{total_arquivos}] SALVO: {trimestre} ({len(df_resultado)} linhas) -> {nome}\")\n",
        "                    else:\n",
        "                        print(f\"[{processados}/{total_arquivos}] Ignorado/Vazio: {nome}\")\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f\"[{processados}/{total_arquivos}] Falha na tarefa {nome}: {exc}\")\n",
        "\n",
        "        conn.close()\n",
        "        print(f\"--- FIM. {sucessos} arquivos processados com sucesso. ---\")\n",
        "\n",
        "# --- EXECUÇÃO ---\n",
        "if __name__ == \"__main__\":\n",
        "    db_nome = 'base_ans_paralela.db'\n",
        "\n",
        "    # Ajuste o número de workers conforme sua internet e CPU\n",
        "    # 8 costuma ser um bom número. Se a internet cair, reduza para 4.\n",
        "    extrator = ExtratorContabilParalelo(db_path=db_nome)\n",
        "    extrator.executar(max_workers=8)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_8Kwf4PPP5pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2364f5-a19a-4982-cbc5-c020ef7c1a0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Mapeando estrutura de pastas em: https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/ ---\n",
            "Anos encontrados: 19. Buscando ZIPs dentro de cada ano...\n",
            "--- Iniciando Download e Processamento de 75 arquivos ---\n",
            "--- Workers Ativos: 8 ---\n",
            "[1/75] SALVO: 2008-T2 (1299 linhas) -> 2008_2_trimestre.zip\n",
            "[2/75] SALVO: 2008-T3 (1296 linhas) -> 2008_3_trimestre.zip\n",
            "[3/75] SALVO: 2008-T1 (1299 linhas) -> 2008_1_trimestre.zip\n",
            "[4/75] SALVO: 2007-T2 (1366 linhas) -> 2007_2_trimestre.zip\n",
            "[5/75] SALVO: 2008-T4 (1290 linhas) -> 2008_4_trimestre.zip\n",
            "[6/75] SALVO: 2007-T1 (1367 linhas) -> 2007_1_trimestre.zip\n",
            "[7/75] SALVO: 2007-T3 (1368 linhas) -> 2007_3_trimestre.zip\n",
            "[8/75] SALVO: 2007-T4 (1354 linhas) -> 2007_4_trimestre.zip\n",
            "[9/75] SALVO: 2009-T1 (1265 linhas) -> 2009_1_trimestre.zip\n",
            "[10/75] SALVO: 2010-T3 (1256 linhas) -> 2010_3_trimestre.zip\n",
            "[11/75] SALVO: 2009-T2 (1273 linhas) -> 2009_2_trimestre.zip\n",
            "[12/75] SALVO: 2010-T1 (1249 linhas) -> 2010_1_trimestre.zip\n",
            "[13/75] SALVO: 2009-T4 (1278 linhas) -> 2009_4_trimestre.zip\n",
            "[14/75] SALVO: 2009-T3 (1277 linhas) -> 2009_3_trimestre.zip\n",
            "[15/75] SALVO: 2010-T2 (1254 linhas) -> 2010_2_trimestre.zip\n",
            "[16/75] SALVO: 2010-T4 (1247 linhas) -> 2010_4_trimestre.zip\n",
            "[17/75] SALVO: 2011-T1 (960 linhas) -> 20120614_2011_1_trimestre.zip\n",
            "[18/75] SALVO: 2012-T1 (963 linhas) -> 20130416_1T2012.zip\n",
            "[19/75] SALVO: 2012-T3 (962 linhas) -> 20130416_3T2012.zip\n",
            "[20/75] SALVO: 2012-T2 (965 linhas) -> 20130416_2T2012.zip\n",
            "[21/75] SALVO: 2011-T3 (964 linhas) -> 20120614_2011_3_trimestre.zip\n",
            "[22/75] SALVO: 2011-T2 (967 linhas) -> 20120614_2011_2_trimestre.zip\n",
            "[23/75] SALVO: 2012-T4 (1216 linhas) -> 20130416_4T2012.zip\n",
            "[24/75] SALVO: 2011-T4 (1231 linhas) -> 20120614_2011_4_trimestre.zip\n",
            "[25/75] SALVO: 2013-T1 (952 linhas) -> 2013-1t.zip\n",
            "[26/75] SALVO: 2013-T2 (953 linhas) -> 2013-2t.zip\n",
            "[27/75] SALVO: 2013-T3 (958 linhas) -> 2013-3t.zip\n",
            "[28/75] SALVO: 2014-T2 (949 linhas) -> 2T2014.zip\n",
            "[29/75] SALVO: 2014-T1 (944 linhas) -> 1T2014.zip\n",
            "[30/75] SALVO: 2014-T3 (947 linhas) -> 3T2014.zip\n",
            "[31/75] SALVO: 2015-T1 (927 linhas) -> 1T2015.zip\n",
            "[32/75] SALVO: 2013-T4 (1189 linhas) -> 2013-4t.zip\n",
            "[33/75] SALVO: 2014-T4 (1170 linhas) -> 4T2014.zip\n",
            "[34/75] SALVO: 2015-T2 (922 linhas) -> 2T2015.zip\n",
            "[35/75] SALVO: 2015-T3 (916 linhas) -> 3T2015.zip\n",
            "[36/75] SALVO: 2016-T2 (904 linhas) -> 2T2016.zip\n",
            "[37/75] SALVO: 2016-T1 (908 linhas) -> 1T2016.zip\n",
            "[38/75] SALVO: 2015-T4 (1142 linhas) -> 4T2015.zip\n",
            "[39/75] SALVO: 2016-T3 (902 linhas) -> 3T2016.zip\n",
            "[40/75] SALVO: 2016-T4 (1115 linhas) -> 4T2016.zip\n",
            "[41/75] SALVO: 2017-T1 (901 linhas) -> 1T2017.zip\n",
            "[42/75] SALVO: 2017-T2 (886 linhas) -> 2T2017.zip\n",
            "[43/75] SALVO: 2017-T3 (880 linhas) -> 3-Trimestre.zip\n",
            "[44/75] SALVO: 2018-T1 (873 linhas) -> 1T2018.zip\n",
            "[45/75] SALVO: 2018-T3 (880 linhas) -> 3T2018.zip\n",
            "[46/75] SALVO: 2017-T4 (1092 linhas) -> 4T2017.zip\n",
            "[47/75] SALVO: 2018-T2 (876 linhas) -> 2T2018.zip\n",
            "[48/75] SALVO: 2019-T1 (854 linhas) -> 1T2019.zip\n",
            "[49/75] SALVO: 2018-T4 (1071 linhas) -> 4T2018.zip\n",
            "[50/75] SALVO: 2019-T2 (865 linhas) -> 2T2019.zip\n",
            "[51/75] SALVO: 2019-T3 (866 linhas) -> 3T2019.zip\n",
            "[52/75] SALVO: 2019-T4 (1047 linhas) -> 4T2019.zip\n",
            "[53/75] SALVO: 2020-T1 (849 linhas) -> 1T2020.zip\n",
            "[54/75] SALVO: 2020-T2 (846 linhas) -> 2T2020.zip\n",
            "[55/75] SALVO: 2020-T3 (839 linhas) -> 3T2020.zip\n",
            "[56/75] SALVO: 2021-T1 (842 linhas) -> 1T2021.zip\n",
            "[57/75] SALVO: 2021-T2 (858 linhas) -> 2T2021.zip\n",
            "[58/75] SALVO: 2021-T3 (860 linhas) -> 3T2021.zip\n",
            "[59/75] SALVO: 2020-T4 (1027 linhas) -> 4T2020.zip\n",
            "[60/75] SALVO: 2022-T1 (833 linhas) -> 1T2022.zip\n",
            "[61/75] SALVO: 2021-T4 (1029 linhas) -> 4T2021.zip\n",
            "[62/75] SALVO: 2022-T2 (835 linhas) -> 2T2022.zip\n",
            "[63/75] SALVO: 2023-T1 (824 linhas) -> 1T2023.zip\n",
            "[64/75] SALVO: 2022-T3 (830 linhas) -> 3T2022.zip\n",
            "[65/75] SALVO: 2023-T2 (829 linhas) -> 2T2023.zip\n",
            "[66/75] SALVO: 2023-T3 (830 linhas) -> 3T2023.zip\n",
            "[67/75] SALVO: 2022-T4 (1005 linhas) -> 4T2022.zip\n",
            "[68/75] SALVO: 2024-T1 (766 linhas) -> 1T2024.zip\n",
            "[69/75] SALVO: 2024-T2 (766 linhas) -> 2T2024.zip\n",
            "[70/75] SALVO: 2023-T4 (997 linhas) -> 4T2023.zip\n",
            "[71/75] SALVO: 2024-T3 (778 linhas) -> 3T2024.zip\n",
            "[72/75] SALVO: 2025-T1 (756 linhas) -> 1T2025.zip\n",
            "[73/75] SALVO: 2025-T2 (760 linhas) -> 2T2025.zip\n",
            "[74/75] SALVO: 2025-T3 (765 linhas) -> 3T2025.zip\n",
            "[75/75] SALVO: 2024-T4 (972 linhas) -> 4T2024.zip\n",
            "--- FIM. 75 arquivos processados com sucesso. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio Extração Dimensão Operadora\n"
      ],
      "metadata": {
        "id": "EOFUUIl2d3WT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observação\n",
        "para realizar a extração da dimensão é necessario rodar esse trecho de codigo na maquina local, pois o google colab contem o IP de fora e essa API so permise realizar requisição em um IP brasileiro"
      ],
      "metadata": {
        "id": "2sviHvWuVtvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImportadorAPI_ANS:\n",
        "    def __init__(self, db_path):\n",
        "        self.db_path = db_path\n",
        "        self.base_url = \"https://dados-abertos-service.pr.ans.gov.br/operadoras\"\n",
        "\n",
        "        # Cria uma sessão que IMPERSONA (finge ser) o Chrome 120\n",
        "        # Isso resolve o problema de Handshake TLS e bloqueio de bot\n",
        "        self.session = requests_curl.Session(impersonate=\"chrome120\")\n",
        "\n",
        "        # Headers adicionais para parecer ainda mais legítimo\n",
        "        self.session.headers.update({\n",
        "            'Accept': 'application/json, text/plain, */*',\n",
        "            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "            'Referer': 'https://dados-abertos-service.pr.ans.gov.br/',\n",
        "            'Origin': 'https://dados-abertos-service.pr.ans.gov.br'\n",
        "        })\n",
        "\n",
        "    def _requisicao_segura(self, url, params=None):\n",
        "        \"\"\"Faz a requisição usando curl_cffi para evitar bloqueios\"\"\"\n",
        "        max_tentativas = 3\n",
        "        for tentativa in range(max_tentativas):\n",
        "            try:\n",
        "                # O curl_cffi não precisa de verify=False geralmente,\n",
        "                # mas mantemos timeout para não travar\n",
        "                response = self.session.get(url, params=params, timeout=30)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    return response.json()\n",
        "                elif response.status_code == 404:\n",
        "                    return None\n",
        "                elif response.status_code in [502, 503, 504]:\n",
        "                    print(f\"   [!] Servidor instável ({response.status_code})... T: {tentativa+1}\")\n",
        "                    time.sleep(3)\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"   [!] Erro {response.status_code} na URL: {url}\")\n",
        "                    return None\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   [!] Erro de conexão (Tentativa {tentativa+1}): {e}\")\n",
        "                time.sleep(2)\n",
        "        return None\n",
        "\n",
        "    def get_classificacoes(self):\n",
        "        print(\"--- Buscando Tabela Auxiliar de Classificações ---\")\n",
        "        url = f\"{self.base_url}/classificacoes\"\n",
        "        dados = self._requisicao_segura(url)\n",
        "\n",
        "        if dados and 'content' in dados:\n",
        "            df = pd.DataFrame(dados['content'])\n",
        "            if not df.empty:\n",
        "                df.rename(columns={'descricao': 'classificacao_descricao_oficial', 'sigla': 'classificacao_sigla'}, inplace=True)\n",
        "            return df\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def get_operadoras_basico(self):\n",
        "        print(\"--- Buscando IDs das Operadoras (Paginação) ---\")\n",
        "        operadoras_lista = []\n",
        "        pagina = 1\n",
        "        tem_dados = True\n",
        "\n",
        "        while tem_dados:\n",
        "            # Feedback visual a cada página para saber que não travou\n",
        "            print(f\"   Lendo página {pagina}...\")\n",
        "\n",
        "            params = {'page': pagina, 'size': 100}\n",
        "            dados = self._requisicao_segura(self.base_url, params=params)\n",
        "\n",
        "            if dados and 'content' in dados and len(dados['content']) > 0:\n",
        "                items = dados['content']\n",
        "                for item in items:\n",
        "                    op = {\n",
        "                        \"registro_ans\": item.get(\"registro_ans\"),\n",
        "                        \"url_detalhe\": item.get(\"_links\", {}).get(\"self\", {}).get(\"href\")\n",
        "                    }\n",
        "                    operadoras_lista.append(op)\n",
        "\n",
        "                if dados.get('last') is True:\n",
        "                    tem_dados = False\n",
        "                else:\n",
        "                    pagina += 1\n",
        "            else:\n",
        "                tem_dados = False\n",
        "\n",
        "        print(f\"   Total de operadoras listadas para consulta: {len(operadoras_lista)}\")\n",
        "        df = pd.DataFrame(operadoras_lista)\n",
        "        if not df.empty:\n",
        "            df.drop_duplicates(subset=['registro_ans'], inplace=True)\n",
        "        return df\n",
        "\n",
        "    def _buscar_detalhe_operadora(self, row):\n",
        "        url = row['url_detalhe']\n",
        "        if not url: return {}\n",
        "\n",
        "        d = self._requisicao_segura(url)\n",
        "\n",
        "        if d:\n",
        "            return {\n",
        "                \"registro_ans\": d.get(\"registro_ans\"),\n",
        "                \"cnpj\": d.get(\"cnpj\"),\n",
        "                \"razao_social\": d.get(\"razao_social\"),\n",
        "                \"nome_fantasia\": d.get(\"nome_fantasia\"),\n",
        "                \"ativa\": d.get(\"ativa\"),\n",
        "                \"registrada_em\": d.get(\"registrada_em\"),\n",
        "                \"descredenciada_em\": d.get(\"descredenciada_em\"),\n",
        "                \"descredenciamento_motivo\": d.get(\"descredenciamento_motivo\"),\n",
        "                \"classificacao_sigla\": d.get(\"classificacao_sigla\"),\n",
        "                \"classificacao_nome\": d.get(\"classificacao_nome\"),\n",
        "                \"representante_nome\": d.get(\"representante_nome\"),\n",
        "                \"representante_cargo\": d.get(\"representante_cargo\"),\n",
        "                \"endereco_logradouro\": d.get(\"endereco_logradouro\"),\n",
        "                \"endereco_numero\": d.get(\"endereco_numero\"),\n",
        "                \"endereco_complemento\": d.get(\"endereco_complemento\"),\n",
        "                \"endereco_bairro\": d.get(\"endereco_bairro\"),\n",
        "                \"endereco_cep\": d.get(\"endereco_cep\"),\n",
        "                \"endereco_municipio_codigo\": d.get(\"endereco_municipio_codigo\"),\n",
        "                \"endereco_municipio_nome\": d.get(\"endereco_municipio_nome\"),\n",
        "                \"endereco_uf_sigla\": d.get(\"endereco_uf_sigla\"),\n",
        "                \"endereco_valido\": d.get(\"endereco_valido\"),\n",
        "                \"telefone_ddd\": d.get(\"telefone_ddd\"),\n",
        "                \"telefone_numero\": d.get(\"telefone_numero\"),\n",
        "                \"fax_ddd\": d.get(\"fax_ddd\"),\n",
        "                \"fax_numero\": d.get(\"fax_numero\"),\n",
        "                \"email\": d.get(\"email_comercial\") or d.get(\"email\")\n",
        "            }\n",
        "        return {}\n",
        "\n",
        "    def processar(self):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1. Classificações\n",
        "        df_classificacoes = self.get_classificacoes()\n",
        "\n",
        "        # 2. Lista Básica\n",
        "        df_basico = self.get_operadoras_basico()\n",
        "\n",
        "        if df_basico.empty:\n",
        "            print(\"Nenhuma operadora encontrada ou erro fatal de conexão.\")\n",
        "            return\n",
        "\n",
        "        # 3. Detalhes em Paralelo\n",
        "        print(\"--- Iniciando Extração Detalhada (Multi-Thread) ---\")\n",
        "        detalhes_lista = []\n",
        "        lista_tarefas = df_basico.to_dict('records')\n",
        "\n",
        "        # Mantendo 8 workers para não abusar\n",
        "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "            future_to_op = {executor.submit(self._buscar_detalhe_operadora, item): item for item in lista_tarefas}\n",
        "\n",
        "            total = len(lista_tarefas)\n",
        "            for i, future in enumerate(as_completed(future_to_op)):\n",
        "                if i % 25 == 0:\n",
        "                    print(f\"   Progresso: {i}/{total} operadoras processadas...\")\n",
        "\n",
        "                res = future.result()\n",
        "                if res:\n",
        "                    detalhes_lista.append(res)\n",
        "\n",
        "        df_final = pd.DataFrame(detalhes_lista)\n",
        "\n",
        "        # 4. Merge Classificações\n",
        "        if not df_classificacoes.empty and 'classificacao_sigla' in df_final.columns:\n",
        "            print(\"--- Validando Classificações ---\")\n",
        "            df_final = pd.merge(df_final, df_classificacoes, on='classificacao_sigla', how='left')\n",
        "\n",
        "        # 5. Salvar\n",
        "        print(\"--- Tratando Dados e Salvando no SQLite ---\")\n",
        "        if df_final.empty:\n",
        "            print(\"Erro: Nenhum detalhe foi coletado.\")\n",
        "            return\n",
        "\n",
        "        df_final = df_final.astype(str)\n",
        "        df_final.replace(['nan', 'None', 'NAT'], None, inplace=True)\n",
        "\n",
        "        df_obj = df_final.select_dtypes(['object'])\n",
        "        df_final[df_obj.columns] = df_obj.apply(lambda x: x.str.strip() if x is not None else x)\n",
        "\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        df_final.to_sql('dim_operadoras', conn, if_exists='replace', index=False)\n",
        "\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_dim_op_reg ON dim_operadoras(registro_ans);')\n",
        "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_dim_op_cnpj ON dim_operadoras(cnpj);')\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        tempo_total = (time.time() - start_time) / 60\n",
        "        print(f\"--- Processo Concluído em {tempo_total:.2f} minutos. Total registros: {len(df_final)} ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    db_nome = 'banco_dim_operadoras.db'\n",
        "    etl = ImportadorAPI_ANS(db_nome)\n",
        "    etl.processar()"
      ],
      "metadata": {
        "id": "jLtDdXOgwxwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O trecho de codigo abaixo serve para juntar os dados da dimensão operadora junto com a base_ans_pararela"
      ],
      "metadata": {
        "id": "v6EE-gB_erLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClonadorComTratamento:\n",
        "    def __init__(self, db_origem, db_destino):\n",
        "        self.db_origem = db_origem\n",
        "        self.db_destino = db_destino\n",
        "        self.tabela = 'dim_operadoras'\n",
        "\n",
        "        # 1. Colunas para EXCLUIR\n",
        "        self.colunas_ignorar = ['_links', 'segmentacoes']\n",
        "\n",
        "        # 2. Colunas para RENOMEAR (De -> Para)\n",
        "        self.mapa_renomeacao = {\n",
        "            'registro_ans': 'registro_operadora',\n",
        "            'classificacao_nome': 'modalidade',\n",
        "            'endereco_municipio_nome': 'cidade',\n",
        "            'endereco_uf_sigla': 'uf',\n",
        "            'representante_nome': 'representante',\n",
        "            'representante_cargo': 'cargo_representante',\n",
        "            'registrada_em': 'data_registro_ans'\n",
        "        }\n",
        "\n",
        "    def _obter_colunas_origem(self):\n",
        "        \"\"\"Lê o nome das colunas do banco original para montar a query dinâmica\"\"\"\n",
        "        conn = sqlite3.connect(self.db_origem)\n",
        "        cursor = conn.cursor()\n",
        "        try:\n",
        "            # Pega informações da tabela (o segundo item da tupla é o nome da coluna)\n",
        "            cursor.execute(f\"PRAGMA table_info({self.tabela})\")\n",
        "            colunas = [row[1] for row in cursor.fetchall()]\n",
        "            return colunas\n",
        "        finally:\n",
        "            conn.close()\n",
        "\n",
        "    def executar(self):\n",
        "        if not os.path.exists(self.db_origem):\n",
        "            print(f\"Erro: Origem {self.db_origem} não encontrada.\")\n",
        "            return\n",
        "\n",
        "        print(f\"--- Iniciando Migração Otimizada com Tratamento ---\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1. Pega as colunas existentes para montar o SQL\n",
        "        colunas_originais = self._obter_colunas_origem()\n",
        "        if not colunas_originais:\n",
        "            print(\"Erro: A tabela de origem parece não existir ou está vazia.\")\n",
        "            return\n",
        "\n",
        "        # 2. Monta a lista de seleção SQL (Select List)\n",
        "        campos_select = []\n",
        "        for col in colunas_originais:\n",
        "            # Regra de Exclusão\n",
        "            if col in self.colunas_ignorar:\n",
        "                continue\n",
        "\n",
        "            # Regra de Renomeação\n",
        "            if col in self.mapa_renomeacao:\n",
        "                novo_nome = self.mapa_renomeacao[col]\n",
        "                campos_select.append(f\"{col} AS {novo_nome}\")\n",
        "            else:\n",
        "                # Mantém o nome original\n",
        "                campos_select.append(col)\n",
        "\n",
        "        query_colunas = \", \".join(campos_select)\n",
        "\n",
        "        # Conexão com Destino\n",
        "        conn = sqlite3.connect(self.db_destino)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        try:\n",
        "            # Configurações de velocidade\n",
        "            cursor.execute(\"PRAGMA synchronous = OFF;\")\n",
        "            cursor.execute(\"PRAGMA journal_mode = MEMORY;\")\n",
        "\n",
        "            print(\"1. Anexando banco de origem...\")\n",
        "            cursor.execute(f\"ATTACH DATABASE '{self.db_origem}' AS origem;\")\n",
        "\n",
        "            print(\"2. Limpando tabela antiga no destino...\")\n",
        "            cursor.execute(f\"DROP TABLE IF EXISTS main.{self.tabela};\")\n",
        "\n",
        "            print(\"3. Executando cópia transformada (SQL Engine)...\")\n",
        "            # AQUI ESTÁ A MÁGICA: O SQL já faz a seleção e renomeação enquanto copia\n",
        "            sql_final = f\"\"\"\n",
        "                CREATE TABLE main.{self.tabela} AS\n",
        "                SELECT {query_colunas}\n",
        "                FROM origem.{self.tabela};\n",
        "            \"\"\"\n",
        "            cursor.execute(sql_final)\n",
        "\n",
        "            print(\"4. Criando índices nas novas colunas...\")\n",
        "            # Atenção: Criamos índices com os NOMES NOVOS\n",
        "            cursor.execute(f\"CREATE INDEX IF NOT EXISTS idx_reg_operadora ON {self.tabela}(registro_operadora);\")\n",
        "\n",
        "            # Verifica se 'cnpj' ainda existe (pois não renomeamos ele, mas ele pode estar na lista)\n",
        "            if 'cnpj' in colunas_originais and 'cnpj' not in self.colunas_ignorar:\n",
        "                 cursor.execute(f\"CREATE INDEX IF NOT EXISTS idx_cnpj ON {self.tabela}(cnpj);\")\n",
        "\n",
        "            conn.commit()\n",
        "            print(\"5. Commit realizado.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro crítico: {e}\")\n",
        "            conn.rollback()\n",
        "        finally:\n",
        "            try:\n",
        "                cursor.execute(\"DETACH DATABASE origem;\")\n",
        "            except:\n",
        "                pass\n",
        "            conn.close()\n",
        "\n",
        "        tempo = time.time() - start_time\n",
        "        print(f\"--- Concluído em {tempo:.4f} segundos ---\")\n",
        "\n",
        "# --- EXECUÇÃO ---\n",
        "if __name__ == \"__main__\":\n",
        "    db_origem = 'banco_dim_operadoras.db'  # Seu banco \"sujo\" (staging)\n",
        "    db_destino = 'base_ans_paralela.db' # Seu banco \"limpo\" (final)\n",
        "\n",
        "    etl = ClonadorComTratamento(db_origem, db_destino)\n",
        "    etl.executar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85wV8fL3YeIz",
        "outputId": "a53b3fac-7871-4545-ae08-5071f197bf44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iniciando Migração Otimizada com Tratamento ---\n",
            "1. Anexando banco de origem...\n",
            "2. Limpando tabela antiga no destino...\n",
            "3. Executando cópia transformada (SQL Engine)...\n",
            "4. Criando índices nas novas colunas...\n",
            "5. Commit realizado.\n",
            "--- Concluído em 0.1039 segundos ---\n"
          ]
        }
      ]
    }
  ]
}